# -*- coding: utf-8 -*-
"""cs4780_kaggleproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11FxvfCUaz8n5MgPfABSgf3aM07IKXAs5

# CS 4780 Kaggle Competition
Through the use of the language Python and its respective known libraries such as Pandas and Scikit-Learn, below are developed machine learning models for the CS 4780 Spring 2024 Kaggle Competition where one must find out if a patient has heart disease or not given several features.
"""

import pandas as pd
from google.colab import drive

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer

drive.mount('/content/gdrive')

"""# Import the the test, train, and validation data through the Pandas library, which can access Google Drive."""

test = pd.read_csv('gdrive/ My Drive/test.csv')
train = pd.read_csv('gdrive/ My Drive/train.csv')
validation = pd.read_csv('gdrive/ My Drive/validation.csv')
test.head()

train.head()

validation.head()

"""# Logistic Regression"""

# Prepare the data
features = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']
X_train = train[features]
y_train = train['label']
X_validation = validation[features]
y_validation = validation['label']
X_test = test[features]

# Impute missing values
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_validation_imputed = imputer.transform(X_validation)
X_test_imputed = imputer.transform(X_test)

# Train a logistic regression model
model = LogisticRegression(max_iter=150)
model.fit(X_train_imputed, y_train)

# Validate the model
y_pred_validation = model.predict(X_validation_imputed)
accuracy = accuracy_score(y_validation, y_pred_validation)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')

# Predict on the test set
test_predictions = model.predict(X_test_imputed)

# Save the predictions to a CSV file
test['label'] = test_predictions
submission = test[['id', 'label']]
submission.to_csv('/content/gdrive/My Drive/predictions_rf.csv'
, index=False)

print(submission.head())

"""# SVM"""

# Define your features and target variable
features = train.columns.difference(['label'])
X_train = train[features]
y_train = train['label']
X_validation = validation[features]
y_validation = validation['label']
X_test = test[features]

# Impute missing values
imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)
X_validation = imputer.transform(X_validation)
X_test = imputer.transform(X_test)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_validation = scaler.transform(X_validation)
X_test = scaler.transform(X_test)

# Create the SVM model with RBF kernel
svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')

# Train the model
svm_model.fit(X_train, y_train)

# Validate the model
y_pred = svm_model.predict(X_validation)
accuracy = accuracy_score(y_validation, y_pred)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')

# Predict on the test set
y_test_pred = svm_model.predict(X_test)

# Create a submission DataFrame
submission = pd.DataFrame({
    'id': test['id'],
    'label': y_test_pred
})

# Save to CSV
submission.to_csv('submission_svm.csv', index=False)
submission.to_csv('/content/gdrive/My Drive/submission_svm.csv')

"""
## Hyperparameter Tuning for SVM
We will use GridSearchCV to optimize the parameters of the SVM to possibly improve its performance.
"""

param_grid = {
    'svc__C': [0.1, 1, 10, 100],
    'svc__gamma': [1, 0.1, 0.01, 0.001]
}

pipeline = make_pipeline(StandardScaler(), SVC(kernel='rbf'))
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)
grid_search.fit(X_train, y_train)

print(f'Best parameters: {grid_search.best_params_}')
print(f'Best cross-validation accuracy: {grid_search.best_score_:.2f}%')

"""
## Random Forest Model
Now let's try a Random Forest classifier to see how it performs in comparison to the SVM.
"""

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_validation)
accuracy_rf = accuracy_score(y_validation, y_pred_rf)
print(f'Random Forest Validation Accuracy: {accuracy_rf * 100:.2f}%')

"""## Optimized Random Forest Model"""

# Setup the parameter grid for Random Forest
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the GridSearchCV object
grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5, scoring='accuracy', verbose=1)

# Fit GridSearchCV
grid_rf.fit(X_train, y_train)

# Best parameters and best score
print("Best parameters:", grid_rf.best_params_)
print("Best cross-validation accuracy: {:.2f}%".format(grid_rf.best_score_ * 100))

# Evaluate on the validation set
y_pred_rf = grid_rf.predict(X_validation)
accuracy_rf = accuracy_score(y_validation, y_pred_rf)
print("Random Forest Validation Accuracy: {:.2f}%".format(accuracy_rf * 100))

"""## XGBoost Model"""

# Prepare the data in DMatrix format which is preferred for XGBoost
dtrain = xgb.DMatrix(X_train, label=y_train)
dval = xgb.DMatrix(X_validation, label=y_validation)

# Set parameters for XGBoost
params = {
    'max_depth': 4,
    'eta': 0.1,
    'objective': 'binary:logistic',
    'eval_metric': 'logloss'
}

# Train the model with early stopping
bst = xgb.train(params, dtrain, num_boost_round=1000, evals=[(dval, 'validation')], early_stopping_rounds=10)

# Predict on validation set using bst.best_iteration to correctly limit the number of trees used
y_pred_xgb = bst.predict(dval, iteration_range=(0, bst.best_iteration + 1))
y_pred_xgb = (y_pred_xgb > 0.5).astype(int)  # Ensure predictions are binary (0 or 1)
accuracy_xgb = accuracy_score(y_validation, y_pred_xgb)
print(f'XGBoost Validation Accuracy: {accuracy_xgb * 100:.2f}%')

"""## Stacking Model"""

# Base models
estimators = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('svc', make_pipeline(StandardScaler(), SVC(random_state=42)))
]

# Stacking classifier
stack_model = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression(),
    cv=5
)

# Fit the stacking classifier
stack_model.fit(X_train, y_train)

# Validate the model
y_pred_stack = stack_model.predict(X_validation)
accuracy_stack = accuracy_score(y_validation, y_pred_stack)
print(f'Stacking Model Validation Accuracy: {accuracy_stack * 100:.2f}%')

"""## LightGBM Model"""

import lightgbm as lgb
from sklearn.metrics import accuracy_score

# Prepare the training and validation data
train_data = lgb.Dataset(X_train, label=y_train)
valid_data = lgb.Dataset(X_validation, label=y_validation, reference=train_data)

# Parameters for LightGBM
params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'num_leaves': 31,
    'learning_rate': 0.1,
    'feature_fraction': 0.95,
    'bagging_fraction': 0.9,
    'bagging_freq': 5,
    'verbose': 0
}

# Train the model
num_round = 100
bst = lgb.train(params, train_data, num_round, valid_sets=[valid_data], callbacks=[lgb.early_stopping(10)])



# Assuming `test` is prepared and does not contain the 'id' or target columns in the features
# Predict on the test set
y_pred_lgb = bst.predict(X_validation, num_iteration=bst.best_iteration)
y_pred_lgb = (y_pred_lgb > 0.5).astype(int)



accuracy_lgb = accuracy_score(y_validation, y_pred_lgb)
print(f'LightGBM Validation Accuracy: {accuracy_lgb * 100:.2f}%')

# # Create a submission DataFrame
# submission = pd.DataFrame({
#     'id': test['id'],
#     'label': y_test_pred
# })

# # Save to CSV
# submission.to_csv('submission.csv', index=False)
# print(submission.head())


# Check the number of samples in the test dataset
print("Number of samples in test dataset:", len(test))

# Check the number of predictions generated
print("Number of predictions:", len(y_pred_lgb))

# Assuming that both the length of IDs and predictions now match

# Create a submission DataFrame
submission = pd.DataFrame({
    'id': test['id'],
    'label': y_test_pred
})

# Save to CSV
submission.to_csv('submission.csv', index=False)
print("Submission saved to 'submission.csv'")
submission.to_csv('/content/gdrive/My Drive/submission.csv')
print(submission.head())